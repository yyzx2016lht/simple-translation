"""AIæ¶¦è‰²çª—å£ä½¿ç”¨çš„å·¥å…·å‡½æ•°å’Œå¸¸é‡"""
import os
import json
import subprocess
from pathlib import Path

# è¯­è¨€æ˜ å°„å­—å…¸
LANGUAGE_MAPPING = {
    "Auto": "è‡ªåŠ¨æ£€æµ‹", 
    "Chinese": "ä¸­æ–‡", 
    "English": "è‹±æ–‡", 
    "Japanese": "æ—¥æ–‡", 
    "Korean": "éŸ©æ–‡", 
    "French": "æ³•æ–‡", 
    "German": "å¾·æ–‡", 
    "Russian": "ä¿„æ–‡", 
    "Spanish": "è¥¿ç­ç‰™æ–‡"
}

def get_installed_ollama_models():
    """è·å–å½“å‰ç”µè„‘å·²å®‰è£…çš„Ollamaæ¨¡å‹åˆ—è¡¨"""
    try:
        # æ‰§è¡Œollama listå‘½ä»¤
        result = subprocess.run(["ollama", "list"], 
                               capture_output=True, 
                               text=True, 
                               check=True)
        
        # è§£æè¾“å‡ºç»“æœ
        lines = result.stdout.strip().split('\n')
        if len(lines) <= 1:  # åªæœ‰æ ‡é¢˜è¡Œï¼Œæ²¡æœ‰æ¨¡å‹
            return []
            
        # éœ€è¦æ’é™¤çš„éå¤§æ¨¡å‹åˆ—è¡¨
        exclude_models = [
            'nomic-embed-text:latest', 
            'nomic-embed-text',
        ]
            
        # æå–æ¨¡å‹åç§°ï¼ˆç¬¬ä¸€åˆ—ï¼‰
        models = []
        for line in lines[1:]:  # è·³è¿‡æ ‡é¢˜è¡Œ
            parts = line.split()
            if parts:  # ç¡®ä¿è¡Œä¸ä¸ºç©º
                model_name = parts[0]  # ç¬¬ä¸€åˆ—æ˜¯æ¨¡å‹åç§°
                # æ£€æŸ¥æ˜¯å¦ä¸ºéœ€è¦æ’é™¤çš„æ¨¡å‹
                if model_name not in exclude_models and not any(model_name.startswith(prefix) for prefix in ["nomic-embed"]):
                    models.append(model_name)
                
        return models
    except subprocess.CalledProcessError:
        print("æ‰§è¡Œollama listå‘½ä»¤å¤±è´¥")
        return []
    except Exception as e:
        print(f"è·å–Ollamaæ¨¡å‹åˆ—è¡¨å‡ºé”™: {e}")
        return []

def load_saved_models():
    """åŠ è½½ç”¨æˆ·ä¿å­˜çš„è‡ªå®šä¹‰æ¨¡å‹åˆ—è¡¨"""
    models_file = Path(os.path.expanduser("~")) / ".translator_models.json"
    
    if models_file.exists():
        try:
            with open(models_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"è¯»å–æ¨¡å‹åˆ—è¡¨å‡ºé”™: {e}")
    
    return []

def save_models(models):
    """ä¿å­˜æ¨¡å‹åˆ—è¡¨åˆ°ç”¨æˆ·ç›®å½•"""
    models_file = Path(os.path.expanduser("~")) / ".translator_models.json"
    
    try:
        with open(models_file, 'w', encoding='utf-8') as f:
            json.dump(models, f)
        return True
    except Exception as e:
        print(f"ä¿å­˜æ¨¡å‹åˆ—è¡¨å‡ºé”™: {e}")
        return False

def process_thinking_tags(text, debug_mode=False):
    """å¤„ç†å¤§æ¨¡å‹è¾“å‡ºä¸­çš„<think></think>æ ‡ç­¾"""
    import re
    
    # å­˜å‚¨åŸå§‹é•¿åº¦ç”¨äºè°ƒè¯•
    original_length = len(text)
    
    # å…ˆå¤„ç†å®Œæ•´çš„æ€è€ƒæ ‡ç­¾ - ä½¿ç”¨éè´ªå©ªæ¨¡å¼
    pattern = r'<think>.*?</think>'
    processed = re.sub(pattern, '', text, flags=re.DOTALL)
    
    # å¤„ç†æœªé—­åˆçš„æ€è€ƒæ ‡ç­¾
    open_tag_pos = processed.find('<think>')
    if open_tag_pos >= 0:
        # åªä¿ç•™<think>ä¹‹å‰çš„å†…å®¹ï¼ŒåŠ ä¸Šæ€è€ƒæç¤º
        processed = processed[:open_tag_pos] + ' [ğŸ¤”æ€è€ƒä¸­...] '
    
    # è°ƒè¯•ä¿¡æ¯
    if debug_mode:
        print(f"åŸå§‹æ–‡æœ¬é•¿åº¦: {original_length}, å¤„ç†åé•¿åº¦: {len(processed)}")
        if original_length != len(processed):
            print(f"ç§»é™¤äº† {original_length - len(processed)} ä¸ªå­—ç¬¦çš„æ€è€ƒå†…å®¹")
    
    return processed